## ğŸŒŸ Overview
PreyFinderX is a fully automated, end-to-end bioinformatics pipeline that takes raw sequencing reads and transforms them into clean, ready-to-use taxonomic identifications without the need for manual processing or complicated intermediate steps. Built specifically for dietary analysis and ecological research, it is optimized to handle both 16S rRNA for bacterial and microbiome profiling, and 12S rRNA for vertebrate metabarcoding, making it ideal for studies ranging from gut microbiome assessments to predatorâ€“prey investigations.

With PreyFinderX, you bring just three things:
   1. Paired-end FASTQ files
   2. Forward & reverse primers
   3. Sample tags/barcodes
From there, the pipeline takes care of everything like demultiplexing, trimming, quality control, OTU clustering, BLAST annotation, and taxonomic assignment, producing a neatly organized taxonomy table that is immediately ready for ecological interpretation, dietary composition summaries, or statistical analysis.

## ğŸ¥— Built for Dietary Studies
Whether youâ€™re identifying fish in a sealâ€™s diet, tracking insect prey in bat guano, or profiling a gut microbiome, PreyFinderX delivers:
  1.  High-confidence OTU assignments for reliable diet composition results
  2.  Fully reproducible outputs for publication-ready analysis
  3.  Organized results by sample for easy downstream processing

## Why PreyFinderX Stands Out
âœ… True one-command automation â€” no juggling 10 different tools
âœ… Dual capability â€” 16S & 12S metabarcoding in one pipeline
âœ… Primer-aware processing â€” maximizes sequence recovery & accuracy
âœ… Direct NCBI integration â€” always works with up-to-date databases
âœ… Reproducible structure â€” consistent output folders for every run

## âš™ï¸ Workflow Overview
The pipeline is built around two scripts:
ğŸ›ï¸ run_pipeline.sh â€” Your control panel
ğŸ“ Enter your primer pair, tag sequences, and analysis parameters.
â–¶ï¸ This calls the main engine to start the run.
ğŸ§  monotrim.sh â€” The core processor
Does all the heavy lifting:
Step	Description	Icon
1ï¸âƒ£ Demultiplex & trim	Uses your tags & primers to split and clean reads	âœ‚ï¸
2ï¸âƒ£ Quality filter	Removes low-quality reads	ğŸ§¹
3ï¸âƒ£ Merge pairs	Joins forward & reverse reads into full amplicons	ğŸ”—
4ï¸âƒ£ Length filter	Discards off-target or short fragments	ğŸ“
5ï¸âƒ£ Dereplicate	Collapses identical reads for faster clustering	ğŸ“¦
6ï¸âƒ£ Remove chimeras	Filters out PCR artifacts	ğŸ§ª
7ï¸âƒ£ Cluster OTUs	Groups similar sequences into taxonomic units	ğŸ§¬
8ï¸âƒ£ Select centroids	Picks representative sequences for BLAST	ğŸ¯
9ï¸âƒ£ BLAST search	Matches sequences against the NCBI nt database	ğŸŒ
ğŸ”Ÿ Top-hit filtering	Keeps only the best match per OTU	ğŸ†
1ï¸âƒ£1ï¸âƒ£ Taxonomy assignment	Converts BLAST hits into species names	ğŸ“–

## ğŸ“‚ File & Folder Structure
After running the pipeline, your results will be organized as follows:
PreyFinderX/
â”œâ”€â”€ core/                     # Core scripts for each step
â”‚   â”œâ”€â”€ Monotrim.sh            # Pre-BLAST processing
â”‚   â”œâ”€â”€ blast_remote.sh        # BLAST OTU centroids
â”‚   â”œâ”€â”€ filter_top_hits.R      # Filter BLAST top hits
â”‚   â”œâ”€â”€ convert_taxonomy.py    # Convert to taxonomy
â”‚   â””â”€â”€ ...
â”œâ”€â”€ Results/
â”‚   â”œâ”€â”€ <sample_tag>/
â”‚   â”‚   â”œâ”€â”€ demux_<tag>_F.fastq
â”‚   â”‚   â”œâ”€â”€ demux_<tag>_R.fastq
â”‚   â”‚   â”œâ”€â”€ trimmed_<tag>_F.fastq
â”‚   â”‚   â”œâ”€â”€ trimmed_<tag>_R.fastq
â”‚   â”‚   â”œâ”€â”€ merged_<tag>.fastq
â”‚   â”‚   â”œâ”€â”€ filtered_<tag>.fastq
â”‚   â”‚   â”œâ”€â”€ filtered_<tag>.fasta
â”‚   â”‚   â”œâ”€â”€ dereplicated_<tag>.fasta
â”‚   â”‚   â”œâ”€â”€ nonchim_<tag>.fasta
â”‚   â”‚   â”œâ”€â”€ otu_centroids_<tag>.fasta
â”‚   â”‚   â”œâ”€â”€ otu_map_<tag>.uc
â”‚   â”‚   â”œâ”€â”€ blast/
â”‚   â”‚   â”‚   â”œâ”€â”€ blast_results_all.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaned_blast_results_<tag>.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ taxonomy_results_<tag>.txt
â”‚   â”‚   â””â”€â”€ ...
â””â”€â”€ README.md

## ğŸ§ª Testing the Pipeline
Why use the test folder?
  1. Quick validation: Confirm your setup is correct and all dependencies work
  2. Learning: See example data formatted exactly as needed
  3. Troubleshooting: Isolate issues by comparing your results with expected outputs

How to run the test?
From the root directory, simply run:
./testrun_pipeline.sh

## âš™ï¸ Requirements
   1. seqkit â€” FASTA/FASTQ manipulation
   2. Cutadapt â€” Primer and adapter trimming
   3. vsearch â€” OTU clustering, chimera detection, dereplication
   4. R + dplyr â€” BLAST hit filtering
   5. Python 3 + Biopython â€” Taxonomy fetching
   6. Internet access â€” for NCBI BLAST & taxonomy retrieval

## ğŸ“¥ Clone the repository
git clone https://github.com/SUBRAMANIAM96/PreyFinderX.git
cd PreyFinderX

## ğŸ”‘ Make scripts executable
chmod +x scripts/*.sh

## ğŸ”‘ To edit the script 
nano metatrimx/run_pipeline.sh

## ğŸš€ Run the pipeline
./run_pipeline.sh config/primers_tags.txt data/raw/ results/

## ğŸ‘¥ Contributors
Subramaniam Vijayakumar
ğŸ“§ Email: subramanyamvkumar@gmail.com
ğŸ”— GitHub: SUBRAMANIAM96



